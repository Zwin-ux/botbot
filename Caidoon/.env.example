# OpenAI Configuration
# Your OpenAI API key - Required for LLM Proxy to generate encounters
AE_LLM_API_KEY=your-openai-api-key-here

# HMAC Secret for Inter-Service Authentication
# Shared secret used to authenticate requests between services
# Generate a secure random string for production
AE_HMAC_SECRET=your-secure-hmac-secret-here

# LLM Model Configuration
# OpenAI model to use for encounter generation
# Default: gpt-4o-mini (recommended for cost-effectiveness)
# Options: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo
AE_LLM_MODEL=gpt-4o-mini

# LLM Temperature Setting
# Controls randomness in AI responses (0.0 = deterministic, 2.0 = very random)
# Default: 0.2 (recommended for consistent encounter generation)
AE_LLM_TEMPERATURE=0.2

# LLM Max Output Tokens
# Maximum number of tokens in the AI response
# Default: 800 (sufficient for most encounters)
AE_LLM_MAX_OUTPUT_TOKENS=800

# Service Ports (Optional - defaults are set in docker-compose.yml)
# LLM_PROXY_PORT=8787
# ENGINE_PORT=8786
# WEB_PORT=3000
# GMOD_SIDECAR_PORT=8788
